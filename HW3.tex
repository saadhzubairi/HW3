\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
% \usepackage{xcolor}
% \usepackage{minted}
% \usepackage{tcolorbox}
% \tcbuselibrary{minted,skins}
% \definecolor{bg}{rgb}{0.95,0.95,0.95}
% \usepackage{tcolorbox}
% \tcbuselibrary{listings,skins}
% \newtcblisting{pythoncode}{
%   listing engine=minted,
%   colback=white!95!gray,
%   colframe=black!50,
%   minted language=python,
%   minted options={linenos, breaklines, fontsize=\small},
%   boxrule=0.4pt, arc=3pt, left=1mm, right=1mm
% }

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\renewcommand{\thesubsection}{\alph{subsection})}
\renewcommand{\thesubsubsection}{\roman{subsubsection}.}

% Title and Author Customization
\title{
    \vspace{3em}
    \textbf{Deep Learning: ECE-7123}\\
    Homework 3
    \vspace{1em}
}

\author{
    Saad Zubairi \\ 
    shz2020 \\
    \vspace{1em}
}

\vspace{1em}
\date{May 2, 2025}

\begin{document}
\maketitle	
\pagebreak

\tableofcontents

\pagebreak

\section{Problem 1: Minimax optimization}

Given the function:

\[
\min_x \max_y f(x, y) = 4x^2 - 4y^2.
\]

Which can be plotted as:

\begin{center}
    \includegraphics[scale=0.5]{Q1/plot.png}
\end{center}

\subsection{Saddle point}

To find the saddle point, we can take the partial derivatives with respect to \(x\) and \(y\):

\[
\nabla f(x, y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right) = (8x, -8y)
\]
Setting \(\nabla f(x, y) = 0\), we solve:
\[
8x = 0 \quad \text{and} \quad -8y = 0
\]

This gives the saddle point at \((x, y) = (0, 0)\).

\subsection{Gradient descent/ascent update rules}

To derive the update rules for gradient descent in \(x\) and gradient ascent in \(y\), we start with the function \(f(x, y) = 4x^2 - 4y^2\). The gradient of \(f(x, y)\) is given by:

\[
\nabla f(x, y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right) = (8x, -8y).
\]

For gradient descent in \(x\), we update \(x\) by moving in the direction opposite to the gradient:

\[
x_{t+1} = x_t - \eta \frac{\partial f}{\partial x}.
\]

Substituting \(\frac{\partial f}{\partial x} = 8x_t\), we get:

\[
x_{t+1} = x_t - \eta (8x_t) = x_t (1 - 8\eta).
\]

For gradient ascent in \(y\), we update \(y\) by moving in the direction of the gradient:

\[
y_{t+1} = y_t + \eta \frac{\partial f}{\partial y}.
\]

Substituting \(\frac{\partial f}{\partial y} = -8y_t\), we get:

\[
y_{t+1} = y_t + \eta (-8y_t) = y_t (1 + 8\eta).
\]

Thus, the update rules are:

\[
x_{t+1} = (1 - 8\eta)x_t, \quad y_{t+1} = (1 + 8\eta)y_t.
\]

\subsection{Convergence of the update rules}

To ensure convergence, we analyze the update rules:

\[
x_{t+1} = (1 - 8\eta)x_t, \quad y_{t+1} = (1 + 8\eta)y_t.
\]

For convergence of \(x_t\) in gradient descent, the factor \((1 - 8\eta)\) must satisfy:

\[
|1 - 8\eta| < 1.
\]

Solving this inequality:

\[
-1 < 1 - 8\eta < 1.
\]

Subtracting 1 from all sides:

\[
-2 < -8\eta < 0.
\]

Dividing through by \(-8\) (reversing the inequality):

\[
0 < \eta < \frac{1}{4}.
\]

For \(y_t\) in gradient ascent, the factor \((1 + 8\eta)\) must satisfy:

\[
|1 + 8\eta| < 1.
\]

Solving this inequality:

\[
-1 < 1 + 8\eta < 1.
\]

Subtracting 1 from all sides:

\[
-2 < 8\eta < 0.
\]

Dividing through by \(8\):

\[
-\frac{1}{4} < \eta < 0.
\]

Since \(\eta\) must be positive for gradient descent and ascent to make sense, we combine the results:

\[
0 < \eta < \frac{1}{4}.
\]

Thus, the range of allowable step sizes is:

\[
\boxed{0 < \eta < \frac{1}{4}}
\]

\subsection{Regular gradient descent on both variables}

To analyze regular gradient descent on both variables \(x\) and \(y\), we update both using the gradient of \(f(x, y)\). The gradient is:

\[
\nabla f(x, y) = (8x, -8y).
\]

The update rules for gradient descent on \(x\) and \(y\) are:

\[
x_{t+1} = x_t - \eta \frac{\partial f}{\partial x}, \quad y_{t+1} = y_t - \eta \frac{\partial f}{\partial y}.
\]

Substituting the partial derivatives:

\[
x_{t+1} = x_t - \eta (8x_t) = x_t (1 - 8\eta),
\]
\[
y_{t+1} = y_t - \eta (-8y_t) = y_t (1 + 8\eta).
\]

Thus, the update rules are:

\[
x_{t+1} = (1 - 8\eta)x_t, \quad y_{t+1} = (1 + 8\eta)y_t.
\]

Dynamics of the updates

\begin{itemize}
    \item For \(x_t\), the factor \((1 - 8\eta)\) determines the convergence. If \(0 < \eta < \frac{1}{4}\), then \(|1 - 8\eta| < 1\), ensuring that \(x_t\) decays to 0.
    \item For \(y_t\), the factor \((1 + 8\eta)\) determines the behavior. Since \((1 + 8\eta) > 1\) for \(\eta > 0\), \(y_t\) grows exponentially, leading to divergence.
\end{itemize}

% \subsubsection{Conclusion}

% Regular gradient descent on both variables does not converge to the saddle point unless \(y_0 = 0\) exactly and remains unperturbed. This is because \(y_t\) diverges due to the exponential growth caused by \((1 + 8\eta) > 1\).

\pagebreak

\section{Vision-Language models}

\subsection*{Overview}
In this experiment, we evaluate three different combinations of image and text encoders for CLIP-style contrastive learning. The goal is to assess how different architectural choices affect the model’s ability to align images and captions.

\subsection*{Model Configurations}
\subsubsection*{Combination 1: ResNet-50 + DistilBERT}
\begin{itemize}
    \item Image Encoder: \texttt{resnet50} (output dim: 2048)
    \item Text Encoder: \texttt{distilbert-base-uncased} (output dim: 768)
    \item Projection Dim: 256
\end{itemize}

\subsubsection*{Combination 2: ResNet-18 + BERT}
\begin{itemize}
    \item Image Encoder: \texttt{resnet18} (output dim: 512)
    \item Text Encoder: \texttt{bert-base-uncased} (output dim: 768)
    \item Projection Dim: 256
\end{itemize}

\subsubsection*{Combination 3: ResNet-34 + RoBERTa}
\begin{itemize}
    \item Image Encoder: \texttt{resnet34} (output dim: 512)
    \item Text Encoder: \texttt{roberta-base} (output dim: 768)
    \item Projection Dim: 256
\end{itemize}

\subsection{Rationale}
The encoder combinations were selected semi-randomly to reflect varying model scales. The purpose is to observe whether larger encoders (e.g., ResNet-50, RoBERTa) yield visibly better alignment without tuning hyperparameters.

\subsection{Implementation Changes}

\subsubsection*{Config Changes}
All modifications are made in the \texttt{config} class.

\begin{verbatim}
    model_name = 'resnet50'
    image_embedding = 2048
    text_encoder_model = 'roberta-base'
    text_tokenizer = 'roberta-base'
    text_embedding = 768
    projection_dim = 512
\end{verbatim}

\subsubsection*{Encoder Changes}
In addition to updating the \texttt{config} class, the encoders were modified throughout the implementation to support RoBERTa. This included:

\begin{itemize}
    \item Updating the forward pass to handle RoBERTa's tokenization and embedding outputs.
    \item Adjusting the projection layers to match the output dimensions of RoBERTa (768) and the specified projection dimension (256).
    \item Updating the \texttt{get\_image\_embeddings} function to use \texttt{AutoTokenizer} for tokenization; compaitble with the updated \texttt{text\_tokenizer}.
\end{itemize}

\subsection{Inference Examples}

\subsubsection{Prompt: ``A cat sitting with a human''}
\vspace{0.5em}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A cat sitting with a human/1.png}
        \caption{\tiny ResNet-50 + DistilBERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A cat sitting with a human/2.png}
        \caption{\tiny ResNet-18 + BERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A cat sitting with a human/3.png}
        \caption{\tiny ResNet-34 + RoBERTa}
    \end{subfigure}
    
\end{figure}

\subsubsection{Prompt: ``A crowd gathered in a park''}
\vspace{0.5em}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A crowd gathered in a park/1.png}
        \caption{\tiny ResNet-50 + DistilBERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A crowd gathered in a park/2.png}
        \caption{\tiny ResNet-18 + BERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A crowd gathered in a park/3.png}
        \caption{\tiny ResNet-34 + RoBERTa}
    \end{subfigure}
    
\end{figure}

\subsubsection{Prompt: ``A dog sitting alone in a park''}
\vspace{0.5em}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A dog sitting alone in a park/1.png}
        \caption{\tiny ResNet-50 + DistilBERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A dog sitting alone in a park/2.png}
        \caption{\tiny ResNet-18 + BERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A dog sitting alone in a park/3.png}
        \caption{\tiny ResNet-34 + RoBERTa}
    \end{subfigure}
    
\end{figure}

\subsubsection{Prompt: ``A man skateboarding in the city''}
\vspace{0.5em}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A man skateboarding in the city/1.png}
        \caption{\tiny ResNet-50 + DistilBERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A man skateboarding in the city/2.png}
        \caption{\tiny ResNet-18 + BERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/A man skateboarding in the city/3.png}
        \caption{\tiny ResNet-34 + RoBERTa}
    \end{subfigure}
    
\end{figure}

\subsubsection{Prompt: ``All kinds of food on a table''}
\vspace{0.5em}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/All kinds of food on a table/1.png}
        \caption{\tiny ResNet-50 + DistilBERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/All kinds of food on a table/2.png}
        \caption{\tiny ResNet-18 + BERT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Q2/All kinds of food on a table/3.png}
        \caption{\tiny ResNet-34 + RoBERTa}
    \end{subfigure}
    
\end{figure}

\subsection{Evaluation}

\subsubsection{Prompt: ``A cat sitting with a human''}
This prompt revealed major limitations across all models.

\begin{itemize}
    \item \textbf{Model A:} Returned dog images—incorrect, but at least animal-adjacent.
    \item \textbf{Model B:} Performed the worst: no animals, minimal context alignment, and often returned scenes with no semantic relevance.
    \item \textbf{Model C:} Came up with a range of unrelated content, including vehicles and humans, with no cats in sight.
\end{itemize}

\textbf{Observation:} The models seem to generalize poorly to cat-related queries, possibly due to underrepresentation in the dataset or visual confusion with dogs.

\subsubsection{Prompt: ``A crowd gathered in a park''}
All models demonstrated some degree of success.

\begin{itemize}
    \item \textbf{Model A:} Retrieved crowd scenes, several of which included greenery and open spaces resembling parks.
    \item \textbf{Model B:} Managed decent crowd representations, albeit with minimal park-like settings.
    \item \textbf{Model C:} Captured the crowd element well but failed to recognize the park context, retrieving urban or event scenes instead.
\end{itemize}

\textbf{Observation:} Crowd detection appears robust, but fine-grained location understanding (e.g., park vs street) varies by model.

\subsubsection{Prompt: ``A dog sitting alone in a park''}
Performance improved noticeably here.

\begin{itemize}
    \item \textbf{Model A:} The most consistent in retrieving visually relevant scenes.
    \item \textbf{Model B:} Adequate but unremarkable.
    \item \textbf{Model C:} Despite earlier underperformance, correctly surfaced a few images matching the "sitting dog" description.
\end{itemize}

\textbf{Observation:} The models exhibit strong prior alignment for the concept of "dog + park," likely due to frequent co-occurrence in training data.

\subsubsection{Prompt: ``A man skateboarding in the city''}
A clear separation in model quality emerged.

\begin{itemize}
    \item \textbf{Model A:} Provided highly accurate results, with multiple images of urban skateboarding scenes.
    \item \textbf{Model B:} More ambiguous—some city scenes, limited motion context.
    \item \textbf{Model C:} Performed poorly, retrieving unrelated activities (including what appeared to be a tennis match).
\end{itemize}

\textbf{Observation:} Motion-related prompts require both object and action recognition; only Model A appears capable of handling this dual requirement reliably.

\subsubsection{Prompt: ``All kinds of food on a table''}
Mixed results with some surprises.

\begin{itemize}
    \item \textbf{Model A:} Showed multiple dining scenes, often including food, people at tables, or floral arrangements. Semantically close, though not perfect.
    \item \textbf{Model B:} Failed entirely, presenting irrelevant images such as trucks and individuals using phones.
    \item \textbf{Model C:} Returned vague or partial matches—some table elements, few actual food visuals.
\end{itemize}

\textbf{Observation:} Abstract or multi-object prompts (e.g., “all kinds of food”) seem more challenging for the smaller models. Model A again showed the strongest contextual alignment.


\subsection{Final Observations}
\begin{itemize}
    \item{Model A (ResNet-50 + DistilBERT):} This model consistently outperformed the others in both scene and object understanding. It handled composite prompts well and recovered semantically aligned images even when fine-grained details were missing. The larger image encoder (ResNet-50) likely contributed to its superior performance, compensating for the relatively smaller text encoder (DistilBERT).
    \item{Model B (ResNet-18 + BERT):} This model lagged significantly behind the others. While BERT is more powerful than DistilBERT, the weak image encoder (ResNet-18) likely limited the overall performance. The smaller image encoder struggled to extract sufficient visual features, leading to poor alignment with text embeddings.
    \item{Model C (ResNet-34 + RoBERTa):} This model was inconsistent—sometimes retrieving promising matches (especially for prompts involving dogs and parks), but often drifting semantically. The larger text encoder (RoBERTa) provided strong language representations, but the mid-sized image encoder (ResNet-34) may have been insufficient to fully leverage this capability, resulting in unstable grounding between image and text modalities.
\end{itemize}

\subsection{Impact of Scaling}

Scaling plays a critical role in the performance of vision-language models. Larger image encoders, such as ResNet-50, provide richer visual feature representations, which are crucial for aligning with text embeddings. Similarly, more powerful text encoders, such as RoBERTa, offer better semantic understanding of textual prompts. However, the benefits of scaling are not uniform across modalities—imbalances between the capacities of the image and text encoders can lead to suboptimal performance. For example, Model C's strong text encoder was not fully utilized due to its mid-sized image encoder.

In general, scaling both the image and text encoders proportionally tends to yield the best results, as seen with Model A. However, this comes at the cost of increased computational requirements, which must be balanced against the desired performance and available resources.

\pagebreak

\section{Generative Adversarial Networks (GANs)}

Following is the implementation of a simple GAN using PyTorch. The code is structured to include the generator and discriminator networks, the training loop, and the evaluation of generated images.

\subsection{Code Implementation}
\href{https://github.com/your-repo-link}{All code files can be found here.}

\subsubsection{\texttt{config.py}}

This file contains the configuration settings for the GAN, including hyperparameters and model architecture details.

\begin{lstlisting}[language=Python, caption={Hyperparameters and Paths}, label={lst:config}, frame=single, basicstyle=\ttfamily\small, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{red!70!black}]
DATA_DIR    = "data"
OUTPUT_DIR  = "outputs"
LOG_DIR     = "logs"

IMG_SIZE    = 28
CHANNELS    = 1
LATENT_DIM  = 100

BATCH_SIZE  = 64
LR          = 1e-4
BETAS       = (0.5, 0.999)

EPOCHS      = 50
SAVE_EPOCHS = {10, 30, 50}

SEED        = 42
\end{lstlisting}

\subsubsection{\texttt{dataset.py}}

\begin{lstlisting}[language=Python, caption={DataLoader Class}, label={lst:config}, frame=single, basicstyle=\ttfamily\small, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{red!70!black}]
    import torch
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader
    import config
    
    # Function to create and return a DataLoader for the FashionMNIST dataset
    def get_dataloader():
    # Define a series of transformations to preprocess the images
    tf = transforms.Compose([
        transforms.Resize(config.IMG_SIZE),  
        transforms.ToTensor(),              
        transforms.Normalize((0.5,), (0.5,)),  
        ])
        
        # Load the FashionMNIST dataset with the specified transformations
        ds = datasets.FashionMNIST(
            root=config.DATA_DIR,  
            train=True,           
            download=True,        
            transform=tf          
            )
            
            # Create and return a DataLoader for the dataset
            return DataLoader(
        ds,
        batch_size=config.BATCH_SIZE,  
        shuffle=True,                  
        num_workers=4,                 
        pin_memory=True,               
    )
\end{lstlisting}

\subsection{\texttt{models.py}}
\begin{lstlisting}[language=Python, caption={Discriminator and Generator Models}, label={lst:models}, frame=single, basicstyle=\ttfamily\scriptsize, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{red!70!black}]
import torch.nn as nn
import config

# Discriminator: A CNN-based model to classify real vs. fake images
class Discriminator(nn.Module):
    def __init__(self, channels=config.CHANNELS, feat=64, slope=0.3, drop=0.3):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(channels, feat, 5, 2, 2, bias=False),  # 1x28x28 -> 64x14x14
            nn.LeakyReLU(slope, inplace=True),
            nn.Dropout(drop),
            nn.Conv2d(feat, feat*2, 5, 2, 2, bias=False),    # 64x14x14 -> 128x7x7
            nn.LeakyReLU(slope, inplace=True),
            nn.Dropout(drop),
        )
        ds = config.IMG_SIZE // 4
        self.fc = nn.Linear(feat*2*ds*ds, 1, bias=False)    # 128x7x7 -> scalar

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.fc(x).view(-1)

# Generator: A CNN-based model to generate images from random noise
class Generator(nn.Module):
    def __init__(self, latent_dim=config.LATENT_DIM, channels=config.CHANNELS, feat=64, slope=0.3):
        super().__init__()
        ds = config.IMG_SIZE // 4
        self.fc = nn.Sequential(
            nn.Linear(latent_dim, feat*4*ds*ds, bias=False),  # 100 -> 256x7x7
            nn.BatchNorm1d(feat*4*ds*ds),
            nn.LeakyReLU(slope, inplace=True),
        )
        self.deconv = nn.Sequential(
            nn.Unflatten(1, (feat*4, ds, ds)),               # Reshape to 256x7x7
            nn.ConvTranspose2d(feat*4, feat*2, 5, 2, 2, output_padding=1, bias=False),
            nn.BatchNorm2d(feat*2),
            nn.LeakyReLU(slope, inplace=True),
            nn.ConvTranspose2d(feat*2, feat, 5, 2, 2, output_padding=1, bias=False),
            nn.BatchNorm2d(feat),
            nn.LeakyReLU(slope, inplace=True),
            nn.ConvTranspose2d(feat, channels, 5, 1, 2, bias=False),
            nn.Tanh(),
        )

    def forward(self, z):
        x = self.fc(z)
        return self.deconv(x)
\end{lstlisting}

\subsection{\texttt{utils.py}}
\begin{lstlisting}[language=Python, caption={Utilities}, label={lst:config}, frame=single, basicstyle=\ttfamily\small, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{red!70!black}]
import os
import torch
import torch.nn as nn
from torchvision.utils import save_image, make_grid
import config

# Initialize weights for Conv2d, ConvTranspose2d, and Linear layers
def weights_init(m):
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):
        nn.init.normal_(m.weight, 0.0, 0.02)

# Save generated samples as a grid image
def save_samples(gen, epoch, fixed_noise):
    gen.eval()  # Set generator to evaluation mode
    # Disable gradient computation
    with torch.no_grad():  
        # Normalize to [0,1]
        imgs = gen(fixed_noise).add(1).div(2)  
        # Create grid of images
        grid = make_grid(imgs, nrow=8)  
        # Ensure output directory exists
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)  
        # Save grid image
        save_image(grid, f"{config.OUTPUT_DIR}/epoch_{epoch}.png")  
    # Restore generator to training mode
    gen.train()  

\end{lstlisting}

\subsection{\texttt{train.py}}
\begin{lstlisting}[language=Python, caption={Training Loop}, label={lst:config}, frame=single, basicstyle=\ttfamily\small, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{red!70!black}]
    import os
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.tensorboard import SummaryWriter
    from tqdm import tqdm
    
    import config
    from dataset import get_dataloader
    from models import Generator, Discriminator
    from utils import weights_init, save_samples
    
    def train():
        # Set random seed for reproducibility
        torch.manual_seed(config.SEED)  
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  
    
        # Load dataset
        dl = get_dataloader()  
        # Initialize generator
        gen  = Generator().to(device)  
        # Initialize discriminator
        disc = Discriminator().to(device)  
        # Apply weight initialization to generator
        gen.apply(weights_init)  
        # Apply weight initialization to discriminator
        disc.apply(weights_init)  
    
        # Optimizer for generator
        opt_g = optim.Adam(gen.parameters(), lr=config.LR, betas=config.BETAS)  
        # Optimizer for discriminator
        opt_d = optim.Adam(disc.parameters(), lr=config.LR, betas=config.BETAS)  
        # Loss function
        criterion = nn.BCEWithLogitsLoss()  
    
        # Fixed noise for generating samples
        fixed_noise = torch.randn(64, config.LATENT_DIM, device=device)  
        # TensorBoard writer for logging
        writer = SummaryWriter(config.LOG_DIR)  
    
        step = 0
        for ep in range(1, config.EPOCHS+1):
            # Progress bar for each epoch
            loop = tqdm(dl, desc=f"Epoch {ep}/{config.EPOCHS}")  
            for real, _ in loop:
                real = real.to(device)
                bs = real.size(0)
                noise = torch.randn(bs, config.LATENT_DIM, device=device)
                fake  = gen(noise)
    
                # Discriminator step
                
                opt_d.zero_grad()
                d_real = disc(real)
                d_fake = disc(fake.detach())
                loss_d = criterion(d_real, torch.ones_like(d_real)) + criterion(d_fake, torch.zeros_like(d_fake))
                loss_d.backward()
                opt_d.step()
    
                # Generator step
                
                opt_g.zero_grad()
                d_fake2 = disc(fake)
                loss_g = criterion(d_fake2, torch.ones_like(d_fake2))
                loss_g.backward()
                opt_g.step()
    
                # Log discriminator loss
                writer.add_scalar("Loss/Discriminator", loss_d.item(), step)  
                # Log generator loss
                writer.add_scalar("Loss/Generator",     loss_g.item(), step)  
                step += 1
                # Update progress bar with losses
                loop.set_postfix(d_loss=loss_d.item(), g_loss=loss_g.item())  
    
            if ep in config.SAVE_EPOCHS:
                # Save generated samples at specific epochs
                save_samples(gen, ep, fixed_noise)  
    
        # Ensure output directory exists
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)  
        # Save generator model
        torch.save(gen.state_dict(), os.path.join(config.OUTPUT_DIR, "generator.pth"))  
        # Save discriminator model
        torch.save(disc.state_dict(), os.path.join(config.OUTPUT_DIR, "discriminator.pth"))  
        # Close TensorBoard writer
        writer.close()  
    
    if __name__ == "__main__":
        # Run training
        train()  
    
    \end{lstlisting}
    
\subsection{\texttt{generate.py}}

\begin{lstlisting}[language=Python, caption={Generation}, label={lst:config}, frame=single, basicstyle=\ttfamily\small, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{red!70!black}]
import torch
from models import Generator  
from utils import save_samples
import config 

def generate(model_path=None, n=64):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  
    G = Generator().to(device)  
    mp = model_path or f"{config.OUTPUT_DIR}/generator.pth"  
    G.load_state_dict(torch.load(mp, map_location=device))  
    # Generate random noise
    noise = torch.randn(n, config.LATENT_DIM, device=device)  
    # Generate and save samples
    save_samples(G, "final", noise)  

if __name__ == "__main__":
    # Run the generate function if script is executed
    generate()  

\end{lstlisting}


\end{document}